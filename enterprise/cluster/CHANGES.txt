2.2.2
-----
o Network exceptions that gets logged are now throttled, i.e. logged every N seconds at most,
  instead of for every occurence. This to remove fix an issue where a tight loop could fill up a log very fast.

2.1.5
-----
o Fixes issue where responses to learn requests from cluster
  members could lead to NullPointerExceptions after topology
  changes
o Fixes issue where the lowest numbered instance would fail
  to leave and rejoin the cluster, if it was not master
  before leaving

2.1.0-RC1
---------
o Fixes a number of issues when multiple leader elections are in flight concurrently

2.1.0-M02
---------
o Throttles debug output of clustering code in messages.log
o Fix issue with cluster bricking if specific message lost when master leaves

2.1.0-M01
---------
o Solves issue with join denied when restarting a failed instance before it times out on the master.
o Instances will now not form clusters on startup unless they managed to contact
  every other instance in initial_hosts setting.

2.0.4
-----
o Adds version marking in election results, solving problems with
  incorrect election outcomes and instances failing to join the cluster
o Slave only instances will not become coordinators under any
  circumstances

2.0.3
-----
o Fixes issue where an instance could revert into SLAVE mode while shutting down.

2.0.2
-----
o Fixes issue where failed instances would not be able to rejoin the cluster after restart
o Throttles output of learn failure messages that could create big messages.log files
o Adds INSTANCE_ID header to every message, solving class of problems where
  reusing a URI would cause denied entry errors
o Introduces throttle in the message logging of cluster transitions

2.0.1
-----
o Fixes message reordering issue
o Fixes a Paxos instance reuse issue
o Cluster join denial now includes cluster configuration
o Fixes issues with proper Payload values passed around

1.9.8
-----
o Adds version marking in election results, solving problems with
 incorrect election outcomes and instances failing to join the cluster
o Slave only instances will not become coordinators under any
 circumstances

1.9.7
-----
o Fixes issue where an instance could revert into SLAVE mode while shutting down.

1.9.5 (2013-11-11)
------------------
o Fixed a bug around channel handling which might lead to messages being delayed and leading
  to false heartbeat failures
o It is now possible to specify 0.0.0.0 for ha.cluster_server and properly wait for messages
  on all interfaces
o Removed all usage of getHostName() that might lead to problems on instances with improperly
  configured DNS
o Fixed election bug which might lead to infinite wait on elections
o Fixed election bug which might lead to votes being discarded and result in the wrong master
  being elected
o Instances no longer expect heartbeats from themselves so they can't timeout on their own
o Instances will now ignore suspicions from ex cluster members
o Instances will now ignore suspicions for themselves

1.9.4 (2013-09-19)
------------------
o Suspicions on failed instances are now printed only once, no longer
  spamming messages.log.
o Fixed issue where a failing cluster instance might delay messages between
  the rest of the instances.

1.9.3 (2013-08-30)
------------------
o Fixes issue which might lead to eternal elections
o UUID for Available messages is now ignored, allowing for upgrades from 1.9.2

1.9.1 (2013-07-16)
------------------
o Fixes bug where a single instance cluster will perpetually try to perform elections

1.9 (2013-05-13)
----------------
o Coordinator doesn't ask itself for snapshots, solving a bug where starting an arbiter first would make the
  cluster unresponsive.

1.9.RC2 (2013-04-30)
--------------------
o Upgrades to Netty 3.6.3
o Elections don't start when a quorum is not available, fixing a certain class of race conditions when quorum is lost
o The joining of an instance is now interpreted as a heartbeat, fixing a class of race conditions when an instance joins
  and immediately dies

1.9.RC1 (2013-04-15)
--------------------
o Removes URL discovery mechanism for clustering
o Moves from URI as an instance id to integers - they must be specified manually. The new setting is
  ClusterSettings.server_id
o Instances can now be replaced if they have marked as fail if a new instance with the same id joins the cluster
o For a cluster to be formed now a majority of instances as configured in initial_hosts must be available
o Fixes around cluster formation from instances concurrently starting up
o Explicit join denial is now possible if an instance tries to join using a server_id already in use

1.9.M05 (2013-03-05)
--------------------
o Election happens for all roles when an instance comes online
o Snapshots are no longer received automatically on cluster join
o Instances starting concurrently will now discover each other instead of each starting
  their own cluster
o Acceptor store now does not grow unbounded and is garbage collected
o Learn requests are made towards and responded from all cluster members
o Cluster server will now by default bind to all interfaces (0.0.0.0)

1.9.M04 (2013-01-17)
--------------------
o Warning is now printed if an instance that is not part of ha.initial_hosts tries to join the cluster
o Snapshots are now prunned when received during cluster joining
o Logging cleanup

1.9.M02 (2012-11-30)
--------------------
o Moved shared logic, commonly referred to as cluster client into a ClusterClient class.
o Reduced amount of threads used by Paxos by using fixed size thread pools
o Paxos cluster membership management improved by introducing memberIsUnavailable and cleanup of roles
o allow_init_cluster setting is now respected

1.9.M01 (2012-10-23)
--------------------
o Initial release
